{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import tqdm\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_data(file):\n",
    "    new_data = pd.read_csv(file)\n",
    "    format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    new_data['date'] = pd.to_datetime(new_data['date'], format=format)\n",
    "    new_data['start'] = pd.to_datetime(new_data['start'], format=format)\n",
    "\n",
    "    x_list = 'Farm,year,month,hour,set'\n",
    "    x_list = x_list + ',wd_cut_left,ws,wd,wind_density,u,v' # Wind Features  # wd_cut_right,quarter,weekday\n",
    "    x_list = x_list +',begin,set_seq_cut,turn,dist_cut,catagory,dist' # Start Features\n",
    "    x_list = x_list + ',wp_hn_1,wp_hn_2,wp_hn_3,wp_hn_4,wp_hn_5,wp_hn_6' # history Features\n",
    "    x_list = x_list + ',ws.angle,ws.angle.p3,ws.angle.p2,ws.angle.p1,ws.angle.n1,ws.angle.n2,ws.angle.n3,ws.angle.p4,ws.angle.p5,ws.angle.p6,ws.angle.n4,ws.angle.n5,ws.angle.n6'\n",
    "    x_list = x_list + ',start_begin'\n",
    "    x_list = x_list + ',ws2.angle,ws2.angle.max,ws2.angle.min'  # 0.1475 #,ws2.angle.std\n",
    "    x_list = x_list + ',ws2.wp_hn_1.mean,ws2.wp_hn_1.max,ws2.wp_hn_1.min' #0.147368 ,ws2.wp_hn_1.std\n",
    "    x_list = x_list + ',day' #,dayofyear,weekofyear,is_month_start'#,0.147108, dayofmonth\n",
    "    x_list = x_list + ',ws3.angle'\n",
    "    x_list = x_list + ',ws48.angle.max,ws48.angle.min,ws48.angle.mean' #,ws48.angle.std\n",
    "    x_list = x_list + ',ws2.ws.mean,ws2.ws.max,ws2.ws.min' #,ws2.ws.std\n",
    "    x_list = x_list + ',ws2.wd.mean,ws2.wd.max,ws2.wd.min' #,ws2.wd.std\n",
    "    x_list = x_list + ',ws2.angle.wp_1,ws2.angle.wp_2,ws2.angle.wp_3,ws2.angle.wp_4,ws2.angle.wp_5,ws2.angle.wp_6,ws2.angle.wp_7'\n",
    "    x_list = x_list + ',ws2.angle_p1,ws2.angle_p1.max,ws2.angle_p1.min' #,ws2.angle_p1.std\n",
    "    x_list = x_list + ',ws2.angle_n1,ws2.angle_n1.max,ws2.angle_n1.min' #,ws2.angle_n1.std\n",
    "    #x_list = x_list + ',week,quarter,weekday'\n",
    "    x_list = x_list + ',ws12.angle.max,ws12.angle.mean,ws12.angle.min' #,ws12.angle.std\n",
    "    x_list = x_list.split(',')\n",
    "    return new_data,x_list,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seqence_data(train_data,sequence_length,offset,seq_interval,x_features,y_features,offset_time):\n",
    "    idxs = [];\n",
    "    for iter in range(0, (train_data.shape[0] - sequence_length - offset), seq_interval):\n",
    "        pred_start_time = train_data.loc[train_data.index.values[iter + sequence_length + offset], 'date'] - offset_time\n",
    "        start_time = train_data.loc[train_data.index.values[iter], 'date']\n",
    "        if (start_time == pred_start_time):\n",
    "            idxs.append(iter)\n",
    "    idxs = np.array(idxs)\n",
    "    train_matrix = train_data[ x_features + y_features ].values\n",
    "    x_Datas,y_Datas = get_sub_sequence_data(train_matrix,idxs,sequence_length,offset)\n",
    "    return x_Datas, y_Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_power_sequence(x_features,train_datas,params,offset = 48):\n",
    "    try:\n",
    "        sequence_length,train_val_split = params['sequence_length'],params['train_set_fraction']\n",
    "        seq_interval,number_catagory,offset_wind = params['seq_interval'],params['number_catagory'],params['offset_wind']\n",
    "        total_x_train,total_y_train = None,None\n",
    "        train_Power_Data = train_datas.copy()\n",
    "        train_Power_Data.sort_values(['date', 'Farm', 'dist'], ascending=[1, 1, 0], inplace=True)\n",
    "        y_features = ['power']\n",
    "        res_Data = dict()\n",
    "        res_Data['x_features'] = x_features\n",
    "        res_Data['y_features'] = y_features\n",
    "        sequence_length = sequence_length - 1\n",
    "        X_Test = None\n",
    "        Y_Test = None\n",
    "        for catagory, NN_data in train_Power_Data.groupby('start_catagory'):\n",
    "            groups_data = NN_data.groupby(NN_data['Farm'])\n",
    "            for idx, group_item in tqdm.tqdm(groups_data):\n",
    "                x_Datas,y_Datas = None,None\n",
    "                sub_group_item = group_item.groupby('date').head(1).reset_index(drop=True).copy()\n",
    "                train_group_item = sub_group_item[sub_group_item['Flag'] == True]\n",
    "                loc_values = sub_group_item[sub_group_item['Flag']==False].index.values\n",
    "                predict_data = np.array([sub_group_item.loc[iter - sequence_length: iter+offset-1, x_features].values for iter in loc_values[range(0,len(loc_values),48)]])\n",
    "                y_data = np.array([sub_group_item.loc[iter: iter + offset-1, y_features].values for iter in loc_values[range(0, len(loc_values), 48)]])\n",
    "                X_Test = predict_data if X_Test is None else np.concatenate([X_Test, predict_data],axis=0)\n",
    "                Y_Test = y_data if Y_Test is None else np.concatenate([Y_Test, y_data], axis=0)\n",
    "                train_data = train_group_item.copy()\n",
    "                train_data.sort_values(['date'], ascending=[1], inplace=True)\n",
    "                train_data = train_data.reset_index(drop=True)\n",
    "                train_data = train_data[train_data['Flag'] == True]\n",
    "                X_data, Y_data = get_seqence_data(train_data,sequence_length,offset,seq_interval,x_features,y_features,offset_time = timedelta(hours=(sequence_length + offset)))\n",
    "                x_Datas = X_data if x_Datas is None else np.concatenate((x_Datas, X_data), axis=0)\n",
    "                y_Datas = Y_data if y_Datas is None else np.concatenate((y_Datas, Y_data), axis=0)\n",
    "                total_x_train = x_Datas if total_x_train is None else np.concatenate( [ total_x_train,x_Datas],axis=0 )\n",
    "                total_y_train = y_Datas if total_y_train is None else np.concatenate([ total_y_train,y_Datas], axis=0 )\n",
    "                print(\"Group: \", idx, total_x_train.shape)    \n",
    "            #total_x_train, total_x_test, total_y_train, total_y_test = train_test_split(total_x_train, total_y_train, test_size=1 - train_val_split,random_state=0)\n",
    "            #res_Data['x_train'], res_Data['y_train'], res_Data['x_test'], res_Data['y_test'] = total_x_train, total_y_train, total_x_test, total_y_test\n",
    "        res_Data['x_train'], res_Data['y_train'] = total_x_train, total_y_train\n",
    "        res_Data['x_test'], res_Data['y_test'] = X_Test,Y_Test\n",
    "        return res_Data\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "#from numba import jit\n",
    "#@jit(nopython=True)\n",
    "def get_power_data(train_val_data,sequence_length,offset ):\n",
    "    x_idxs_power = np.array([-2]);\n",
    "    x_idxs_wind = np.array(list(range(0, train_val_data.shape[1] - 1)))\n",
    "    x_idxs_wind2 = np.array( list(range(0, train_val_data.shape[1] - 2)) + [-1 ])\n",
    "    y_idxs = np.array([-1]);\n",
    "\n",
    "    X_data = np.concatenate([train_val_data[0:sequence_length, x_idxs_wind],train_val_data[(offset + sequence_length):(\n",
    "            offset + 2 * sequence_length), x_idxs_wind]], axis = 0)\n",
    "\n",
    "\n",
    "   # X_data = train_val_data[0:sequence_length,x_idxs_wind ]\n",
    "    X_data = train_val_data[0:(sequence_length+offset), x_idxs_wind]\n",
    "    X_data = np.concatenate( [train_val_data[0:(sequence_length), x_idxs_wind2],train_val_data[(sequence_length):(sequence_length+offset), x_idxs_wind]],axis=0)\n",
    "    Y_data = train_val_data[sequence_length:  (sequence_length + offset), y_idxs[0]]\n",
    "    return X_data, Y_data;\n",
    "\n",
    "    \n",
    "def get_sub_sequence_data(train_matrix,idxs,sequence_length,offset):\n",
    "    num_len = idxs.shape[0]\n",
    "    x_Datas = np.empty(shape=( num_len, sequence_length+offset, train_matrix.shape[1] - 1), dtype=np.float64)\n",
    "    y_Datas = np.empty(shape=( num_len, offset), dtype=np.float64)\n",
    "    idx = 0\n",
    "    for iter in idxs:\n",
    "        train_val_data = train_matrix[iter: sequence_length + iter + offset]\n",
    "        x_Datas[idx], y_Datas[idx] = get_power_data(train_val_data, sequence_length, offset)\n",
    "        idx = idx + 1\n",
    "    return x_Datas,y_Datas;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def ISO_Forestclustering(X,outliers_fraction = 0.01,show=False):\n",
    "    anomaly_algorithms = IsolationForest(contamination=outliers_fraction,\n",
    "                                         random_state=42)\n",
    "    y_pred = anomaly_algorithms.fit(X).predict(X)\n",
    "    if show:\n",
    "        colors = np.array(['#377eb8', '#ff7f00'])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])\n",
    "        plt.show()\n",
    "    return y_pred\n",
    "\n",
    "def KNN_Clustering(X,x_list,min_sample,n_clusters=100,show=False):\n",
    "    if False:\n",
    "        wcss = []\n",
    "        for i in range(2, 200,5):\n",
    "            kmeans = KMeans(n_clusters=i, init='k-means++', random_state=0)\n",
    "            kmeans.fit(X)\n",
    "            wcss.append(kmeans.inertia_)\n",
    "        # Visualizing the ELBOW method to get the optimal value of K\n",
    "        plt.plot(range(2, 200,5), wcss)\n",
    "        plt.title('The Elbow Method')\n",
    "        plt.xlabel('no of clusters')\n",
    "        plt.ylabel('wcss')\n",
    "        plt.show()\n",
    "    kmeansmodel = KMeans(n_clusters=n_clusters, init='k-means++', random_state=0)\n",
    "    y_kmeans = kmeansmodel.fit_predict(X)\n",
    "    cluser_label = \"Cluster 1\"\n",
    "    cluser_label2 = \"Cluster 2\"\n",
    "    X['cluster'] = 1\n",
    "    for i in range(n_clusters):\n",
    "        if X.values[y_kmeans == i, 0].shape[0] <= min_sample:\n",
    "            X.loc[X.index[y_kmeans == i],'cluster'] = 0\n",
    "            #plt.scatter( X.values[y_kmeans == i, 0], X.values[y_kmeans == i, 1], s=10, c='blue',label= cluser_label) #, label='Cluster {}'.format(1))  # c='red',\n",
    "            cluser_label = None\n",
    "        else:\n",
    "            #plt.scatter(X.values[y_kmeans == i, 0], X.values[y_kmeans == i, 1], s=10, c='green',label = cluser_label2) #, label='Cluster {}'.format(2))  # c\n",
    "            cluser_label2 = None\n",
    "        # plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s=10, c='blue', label='Cluster 2')\n",
    "    #plt.title('Clusters of Wind Component')\n",
    "    #plt.xlabel(x_list[0])\n",
    "    #plt.ylabel(x_list[1])\n",
    "    #plt.legend()\n",
    "    #save_file = None #join('core','image','cluster')\n",
    "    #if save_file is not None:\n",
    "    #    plt.savefig(save_file, dpi=300, pad_inches = 0,bbox_inches='tight')\n",
    "    #plt.show()\n",
    "    return X\n",
    "\n",
    "def pre_process(train_data,y_list=\"TARGETVAR\", n_clusters = 50,min_sample = 20):\n",
    "    ### First Find Abnormal Points\n",
    "    X_list = ['u', 'v']  # ,'V100','U100'\n",
    "    zero_output = train_data.loc[train_data[y_list]==0,:]\n",
    "    X = zero_output.loc[:,X_list].copy()\n",
    "    clusting = \"kmeans\"\n",
    "    if clusting == \"ISO_Forestclustering\":\n",
    "        ISO_Forestclustering(X)\n",
    "    if clusting == \"DBSCAN\":\n",
    "        outlier_detection = DBSCAN(\n",
    "            eps=.1,\n",
    "            metric=\"euclidean\",\n",
    "        min_samples = 5,\n",
    "                      n_jobs = -1)\n",
    "        clusters = outlier_detection.fit_predict(X)\n",
    "        from matplotlib import cm\n",
    "        cmap = cm.get_cmap(\"Set1\")\n",
    "        X = zero_output.loc[:, X_list]\n",
    "        X.plot.scatter(x=X_list[0], y =X_list[1], c = clusters, cmap = cmap,colorbar = False)\n",
    "        plt.show()\n",
    "    else:\n",
    "        X = KNN_Clustering(X,X_list,n_clusters=n_clusters,min_sample = min_sample)\n",
    "        zero_output.loc[:, 'cluster'] = X['cluster']\n",
    "        train_data.loc[zero_output.index,'cluster'] =  X['cluster']\n",
    "        Lag_offset = 1\n",
    "        ### Time-dependent  [1,2]\n",
    "        zero_output['pre_TimeDiff'] = (zero_output['date'] - zero_output['date'].shift(periods=Lag_offset)).astype('timedelta64[h]')\n",
    "        zero_output['next_TimeDiff'] = (zero_output['date'] - zero_output['date'].shift(periods=-Lag_offset)).astype('timedelta64[h]')\n",
    "\n",
    "        noise_data = zero_output[ (zero_output['cluster']==0 ) ] #  & (zero_output['next_TimeDiff'] != -1) & (zero_output['pre_TimeDiff'] != 1)]\n",
    "        print(noise_data.shape)\n",
    "\n",
    "        train_data.loc[noise_data.index,y_list] = np.nan\n",
    "        train_data = train_data.interpolate(method=\"cubic\")\n",
    "        return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(epochs,loss,val_loss,name):\n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    # b is for \"solid blue line\"\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss_'+ name)\n",
    "    plt.legend(['Training loss','Validation loss'])\n",
    "    plt.show()\n",
    "    #plt.clf()  # clear figure\n",
    "    #plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    #plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    #plt.title('Training and validation accuracy')\n",
    "    #plt.xlabel('Epochs')\n",
    "    #plt.ylabel('Accuracy')\n",
    "    #plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, BatchNormalization,Bidirectional, Dropout,Flatten,Input\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler,EarlyStopping, ModelCheckpoint,TensorBoard\n",
    "import datetime as dt\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from keras.layers import Layer\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "\n",
    "class Reverse_Layer(Layer):\n",
    "    def __init__(self,\n",
    "                 **kwargs):\n",
    "        super(Reverse_Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.axis = len(input_shape)-1\n",
    "        super(Reverse_Layer,self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = K.reverse(inputs,axes = self.axis )\n",
    "        return output\n",
    "\n",
    "weight_decay = 1e-4\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 81:\n",
    "        return 0.1\n",
    "    if epoch < 122:\n",
    "        return 0.1\n",
    "    return 0.1\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt( K.mean(K.square(y_pred - y_true), axis=-1) + 1e-8)\n",
    "\n",
    "\n",
    "def root_mean_category(y_true,y_pred):\n",
    "    y_pred_arg, y_true_arg = K.argmax(y_pred,axis=1),K.argmax(y_true,axis=1)\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1) + 1e-8)\n",
    "\n",
    "def my_func(arg):\n",
    "  arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n",
    "  return tf.matmul(arg, arg) + arg\n",
    "\n",
    "def rever_matrix(x):\n",
    "    x2 = K.reverse(x, axes= len(x.shape)-1 )\n",
    "    print( x2 )\n",
    "    return x2\n",
    "class NN_Net:\n",
    "\n",
    "    def __init__(self,params,input,reverse = None):\n",
    "        self.params = params\n",
    "        self.batch_sizes = params['batch_size']\n",
    "        self.model = Sequential()\n",
    "        self.stack_layers(params['layer'],params['NN_model'],reverse=reverse)\n",
    "        self.len_train = len(input)\n",
    "        #self.build_model(shape)\n",
    "\n",
    "    def stack_layers(self,layers,model_param,reverse = None,kernel_initializer='normal'): #glorot_uniform\n",
    "        for layer in layers:\n",
    "            # create the layer\n",
    "            if layer['type'] == 'input':\n",
    "                input_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
    "                size = layer['size'] if 'size' in layer else None\n",
    "                l = Dense(layer['size'], input_dim=input_dim, kernel_initializer=kernel_initializer,\n",
    "                          kernel_regularizer=l2(0.001), name=layer.get('name'))\n",
    "            elif layer['type'] == 'softplus_reg':\n",
    "                l = Dense(layer['size'], activation='softplus', kernel_initializer=kernel_initializer,\n",
    "                          kernel_regularizer=l2(0.001), name=layer.get('name'))\n",
    "            elif layer['type'] == 'softmax':\n",
    "                l = Dense(layer['size'], activation='softmax', kernel_initializer=kernel_initializer,\n",
    "                          kernel_regularizer=l2(0.001), name=layer.get('name'))\n",
    "            elif layer['type'] == 'tanh':\n",
    "                l = Dense(layer['size'], activation='tanh', kernel_initializer=kernel_initializer,\n",
    "                          kernel_regularizer=l2(0.001), name=layer.get('name'))\n",
    "            elif layer['type'] == 'relu':\n",
    "                l = Dense(layer['size'], activation='relu', kernel_initializer=kernel_initializer,\n",
    "                          kernel_regularizer=l2(0.001), name=layer.get('name'))\n",
    "            elif layer['type'] == 'selu':\n",
    "                l = Dense(layer['size'], activation='selu', kernel_initializer=kernel_initializer,\n",
    "                          kernel_regularizer=l2(0.001), name=layer.get('name'))\n",
    "            elif layer['type'] == 'BatchNormalization':\n",
    "                l = BatchNormalization(name=layer.get('name'))\n",
    "            elif layer['type'] == 'dropout':\n",
    "                l = Dropout(layer['rate'])\n",
    "            elif layer['type'] == 'flatten':\n",
    "                l = Flatten()\n",
    "            elif layer['type'] == 'lstm':\n",
    "                go_backwards = layer['go_backwards'] if 'go_backwards' in layer else False\n",
    "                #reverse =  True if go_backwards else None\n",
    "                input_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n",
    "                input_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
    "                #net_input = Input(shape=(input_timesteps, input_dim), name='net_input')\n",
    "                return_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
    "                l =  LSTM(layer['neurons'], recurrent_dropout=0.2,input_shape=(input_timesteps, input_dim),\n",
    "                         kernel_initializer='lecun_normal', return_sequences=return_seq,go_backwards=go_backwards);\n",
    "            elif layer['type'] == 'dense':\n",
    "                l = Dense( layer['size'], activation =layer['activation'])\n",
    "            else:\n",
    "                raise ValueError(\"Invalid layer type '{}'\".format(layer['type']))\n",
    "            self.model.add(l)\n",
    "        if( reverse is not None):\n",
    "            self.model.add( Reverse_Layer(name='reverse'))\n",
    "        for name,item in model_param.items():\n",
    "            if( name == \"optimizer\"):\n",
    "                if( item['type']==\"sgd\" ):\n",
    "                    optimizer = optimizers.SGD(lr=item['lr'], momentum=0.9, nesterov=True, clipnorm=1.0)\n",
    "                elif(item['type'] == \"sgd\"):\n",
    "                    optimizer = \"adm\"\n",
    "            elif( name ==\"metrics\"):\n",
    "                metrics = item['metrics']+ [root_mean_squared_error]\n",
    "            elif(name==\"loss\"):\n",
    "                loss = item['loss'] if item['loss'] is not None else root_mean_squared_error\n",
    "        self.model.compile(loss = loss, optimizer=optimizer, metrics=metrics)\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def build_model(self,input_shape):\n",
    "        # MLP models\n",
    "        model1 = Sequential()\n",
    "        model1.add(LSTM(48,recurrent_dropout=0.2, input_shape= input_shape , kernel_initializer='lecun_normal', return_sequences=False))\n",
    "        model1.add(Dropout(0.2))\n",
    "        model1.add(Dense(100,activation='softmax'))\n",
    "        model1.add(Dropout(0.5))\n",
    "        # model1.add(Dense(20,activation='elu'))\n",
    "        # model1.add(Dense(1,activation='linear'))\n",
    "        #sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True, clipnorm=1.0)\n",
    "        model1.compile(loss= root_mean_squared_error, optimizer='adam',metrics=['accuracy',root_mean_squared_error])\n",
    "        self.model = model1\n",
    "\n",
    "    def train(self, x, y,x_test,y_test,name,load_models=False):\n",
    "\n",
    "        epochs = self.params['epoch_number']\n",
    "        batch_size = self.params['batch_size']\n",
    "        save_dir = self.params['save_dir']\n",
    "        #if (load_models):\n",
    "        #    save_h5 = os.path.join(save_dir,'22052019-183011-e2000_wind_power.h5')\n",
    "        #    self.model.load_weights(save_h5)\n",
    "        print('[Model] Training Started')\n",
    "        print('[Model] %s epochs, %s batch size' % (epochs, batch_size))\n",
    "        save_fname = os.path.join(save_dir, '%s_%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'),name))\n",
    "        self.best_weight_dir = os.path.join(save_dir, '%s_%s.hdf5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'),name))\n",
    "        model_checkpoint = ModelCheckpoint(self.best_weight_dir, monitor=\"val_loss\",\n",
    "                                           save_best_only=True, save_weights_only=True,\n",
    "                                           verbose=1)\n",
    "            \n",
    "        # set callback\n",
    "        tb_cb = TensorBoard(log_dir= save_dir, histogram_freq=0)\n",
    "        change_lr = LearningRateScheduler(scheduler)\n",
    "        callbacks = [tb_cb,change_lr,\n",
    "            EarlyStopping(monitor='loss', patience=30),\n",
    "            model_checkpoint\n",
    "            #ModelCheckpoint(filepath=save_fname, save_best_only=False, mode='auto', period=10),\n",
    "        ]\n",
    "        self.history = self.model.fit(x,y,\n",
    "            validation_data=(x_test, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks)\n",
    "        self.model.save(save_fname)\n",
    "        print('[Model] Training Completed. Model saved as %s' % save_fname)\n",
    "        self.model.load_weights(self.best_weight_dir)\n",
    "        scores = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        predicted_data = self.model.predict(x_test)\n",
    "        #plot_Predict(y_test, predicted_data,name)\n",
    "        #print(\"Accuracy: %.2f%%\" % (scores))\n",
    "        self.show_histoty(epochs,name)\n",
    "        return scores\n",
    "        \n",
    "        \n",
    "\n",
    "    def load_model(self,name):\n",
    "        save_dir = self.params['save_dir']\n",
    "        filelist = self.file_name( save_dir)\n",
    "        find_rules = '.*'+ name + '.h5$'\n",
    "        filenames = [re.findall(find_rules, file) for i, file in enumerate(filelist['name']) if\n",
    "                     (file.endswith((name + '.h5')))]\n",
    "        save_h5 = os.path.join(save_dir, filenames[0][0])\n",
    "        self.model= load_model(save_h5,custom_objects={'root_mean_squared_error': root_mean_squared_error});\n",
    "\n",
    "    def evaluat(self,x_test,y_test,y,name):\n",
    "        #y_test = self.scaler.transform(y_test)\n",
    "        #y_test = (y_test - self.mean()) / (self.max - self.min)\n",
    "        model_dir = self.params['save_dir']\n",
    "        Jsonfile = model_dir + '/22052019-183011-e2000_wind_power.h5'\n",
    "        model  = load_model(Jsonfile)\n",
    "        model  = model.load_weights(Jsonfile)\n",
    "        #scores = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        #print(\"Accuracy: %.2f%%\" % (scores))\n",
    "        predicted_data = self.model.predict(x_test)\n",
    "        print(predicted_data)\n",
    "        plot_Predict(y_test,predicted_data)\n",
    "\n",
    "    def show_histoty(self,epochs,name):\n",
    "        history_dict = self.history.history\n",
    "        #acc = history_dict['acc']\n",
    "        #val_acc = history_dict['val_acc']\n",
    "        loss = history_dict['loss']\n",
    "        val_loss = history_dict['val_loss']\n",
    "        epochs = np.arange(0, epochs)\n",
    "        #acc_values = history_dict['acc']\n",
    "        #val_acc_values = history_dict['val_acc']\n",
    "        plot_history(epochs,loss,val_loss,name)\n",
    "\n",
    "    def predict(self,data,time,params,name,x_test,y_test):\n",
    "        model_dir = self.params['save_dir']\n",
    "        filelist = self.file_name(model_dir)\n",
    "        find_rules = '-(.+?)-.*.h5$'\n",
    "        filenames = [ re.findall(find_rules,file) for i, file in enumerate(filelist['name']) if( file.endswith( ( name+'.h5')))];\n",
    "        filename = name + '.h5'\n",
    "        Jsonfile = model_dir+'/' + filename\n",
    "        to_file = model_dir+'/' + \"NetModel.png\"\n",
    "        model = load_model(Jsonfile)\n",
    "        model.load_weights(Jsonfile)\n",
    "        # model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "        # print(self.model.summary())\n",
    "        #plot_model(self.net, to_file='', show_shapes=True)\n",
    "        sequence_length = params['sequence_length']\n",
    "        nsteps = len(data)-sequence_length\n",
    "        y_real=[]; y=[]; y2=[]\n",
    "        y_2 = model.predict(x_test)\n",
    "        y_max = params['max_features'][name]\n",
    "        for i in range(nsteps):\n",
    "            x_data,y_data = self.get_sequence_data(data,i,sequence_length)\n",
    "            y_pred = model.predict(x_data)\n",
    "            y.append(y_pred[0]*y_max)\n",
    "            y2.append(y_pred[0][0]*y_max)\n",
    "            y_real.append(y_data*y_max)\n",
    "            data[ i + sequence_length,1]= y_pred\n",
    "        #plot_Predict_time(time['time'],y_real,y,name)\n",
    "        data_frame = pd.DataFrame({'time': time['time'], 'predict': y2})\n",
    "        #print(name + \" 预测MAPE: \" + self.mean_absolute_percentage_error(y_real, y))\n",
    "        return data_frame\n",
    "\n",
    "    def predict2(self,data,time,params,name,x_test,y_test):\n",
    "        model_dir = self.params['save_dir'];\n",
    "        filelist = self.file_name(model_dir);\n",
    "        find_rules = '-(.+?)-.*.h5$';\n",
    "        filenames = [ re.findall(find_rules,file) for i, file in enumerate(filelist['name']) if( file.endswith( ( name+'.h5')))];\n",
    "        filename ='23052019-093203_wind_power_checkpoint-40-0.01.h5';\n",
    "        Jsonfile = model_dir+'/' + filename;\n",
    "        to_file = model_dir+'/' + \"NetModel.png\";\n",
    "        model = load_model(Jsonfile);\n",
    "        model.load_weights(Jsonfile);\n",
    "        #plot_model(model, to_file=to_file, show_shapes=True);\n",
    "        y_max = params['max_features']['power'];\n",
    "        sequence_length = params['sequence_length'];\n",
    "        y_predict = model.predict(x_test);\n",
    "        y_predict = y_predict* y_max;\n",
    "        y_test = y_test *y_max;\n",
    "        #plot_Predict_time(time['time'],y_test,y_predict,name)\n",
    "        print(name + \" 预测MAPE: \" + self.mean_absolute_percentage_error(y_test, y_predict));\n",
    "\n",
    "    def get_sequence_data(self,data_item, idx,sequence_length):\n",
    "        X_data = []\n",
    "        Y_data = []\n",
    "        x_idxs = list(range(0, data_item.shape[1] - 1))\n",
    "        y_idxs = [data_item.shape[1] - 1]\n",
    "        X_data.append(np.concatenate((data_item[idx + 1:(idx + sequence_length), x_idxs],\n",
    "                                          data_item[idx:(idx + sequence_length - 1), y_idxs]), axis=1))\n",
    "            # X_data.append( data_item[ i:(i+sequence_length),[0,1]] )\n",
    "        Y_data=(data_item[idx + sequence_length, y_idxs ])\n",
    "        # train data # test data\n",
    "        X_data = np.array(X_data)\n",
    "        Y_data = np.array(Y_data)\n",
    "        return X_data, Y_data\n",
    "\n",
    "    def normalized(self,x):\n",
    "        x = (self.x - self.min)/()\n",
    "\n",
    "    def reshape_data(self,data):\n",
    "        shape_dim = data.shape[1]*data.shape[2]\n",
    "        data = np.reshape(data,(-1,shape_dim))\n",
    "        return data\n",
    "\n",
    "    def file_name(self,file_dir):\n",
    "        filenames = {\n",
    "            'file': [],\n",
    "            'name': []\n",
    "        }\n",
    "        for root, dirs, files in os.walk(file_dir):\n",
    "            print(root)  # current dir \n",
    "            for file in files:\n",
    "                filenames['file'].append(root + '\\\\' + file)\n",
    "                filenames['name'].append(file)\n",
    "            return filenames\n",
    "\n",
    "    def mean_absolute_percentage_error(self,y_true, y_pred):\n",
    "        # y_true, y_pred = y_true, np.array(y_pred)\n",
    "        sum = 0; n=0;\n",
    "        for index, item in enumerate(y_true):\n",
    "            if( abs(y_true[index])>0):\n",
    "                sum = sum + abs(y_true[index] - y_pred[index]) / y_true[index]\n",
    "                n = n+1\n",
    "        if(n>0):\n",
    "            sum = sum[0] / n\n",
    "        return str(sum * 100)\n",
    "\n",
    "def NN_predict_data(model_data,res_data,params,Limit_Number=None):\n",
    "    sequence_length = params['sequence_length']\n",
    "    train_data, benchmark_data = res_data['train_data'], res_data['benchmark_data']\n",
    "    wind_datas, wind_train_Datas = res_data['wind_datas'], res_data['wind_train_Data']\n",
    "\n",
    "    x_features = ['Farm', 'year', 'month', 'hour']\n",
    "    farm_features = res_data['farm_feature']\n",
    "\n",
    "    if (Limit_Number is None):\n",
    "        start_time = benchmark_data['date']\n",
    "    else:\n",
    "        start_time = benchmark_data[:Limit_Number]['date']\n",
    "    sequence_length = sequence_length - 1\n",
    "    benchmark_data.is_copy = False\n",
    "    data_arrays = {}\n",
    "    temp_dict = {}\n",
    "    other_dict = {}\n",
    "\n",
    "    for idx, item in enumerate(tqdm.tqdm(start_time)):\n",
    "        offset = idx % sequence_length\n",
    "        new_data = benchmark_data[benchmark_data['date'] == item].copy()\n",
    "        for farmidx, feacture in enumerate(farm_features):\n",
    "            if (offset == 0):\n",
    "                data_array = train_data[farmidx][train_data[farmidx]['date'] < item]\n",
    "                data_array = data_array[-sequence_length:]\n",
    "                other_dict[feacture] = data_array[x_features].values\n",
    "                data_array = data_array[ farm_features ].values\n",
    "                data_arrays[feacture] = data_array; #history features\n",
    "                \n",
    "                temp_dict[feacture] = {}\n",
    "                wind_data = wind_datas[farmidx]\n",
    "                wind_train_Data = wind_train_Datas[farmidx]\n",
    "                # temp_wind_data in the future\n",
    "                temp_wind_data = wind_data[wind_data['wind_date'] < item]\n",
    "                temp_dict[feacture]['temp_wind_data'] = temp_wind_data[-sequence_length:]\n",
    "                # wind_array previous features\n",
    "                wind_array = wind_train_Data[wind_train_Data['new_date'] < item][-sequence_length + 1:]\n",
    "                temp_dict[feacture]['wind_array'] = wind_array.drop(columns=['wind_date', 'hors', 'new_date']).values;\n",
    "\n",
    "            temp_wind_data = temp_dict[feacture]['temp_wind_data'] # NWP numerical weather forcasting\n",
    "            wind_array = temp_dict[feacture]['wind_array']  # history features\n",
    "            other_feature_array  = other_dict[feacture]\n",
    "\n",
    "            temp = temp_wind_data[temp_wind_data['new_date'] == item]  # NWP\n",
    "            temp = temp.drop(columns=['wind_date', 'hors', 'new_date']).values\n",
    "            wind_array = np.concatenate((wind_array, temp), axis=0)\n",
    "            temp_dict[feacture]['wind_array'] = wind_array\n",
    "\n",
    "            x_list = ['date','year', 'month', 'hour'] + [feacture]\n",
    "            temp_farm = new_data[x_list].copy()\n",
    "            temp_farm.loc[temp_farm.index._values,'Farm'] = farmidx + 1\n",
    "            temp_farm = temp_farm.rename(columns={feacture: \"power\"})\n",
    "            new_data_temp =  temp_farm[x_features].values\n",
    "            other_feature_array = np.concatenate( (other_feature_array,new_data_temp),axis=0)\n",
    "            other_dict[feacture] = other_feature_array\n",
    "\n",
    "            data_array = data_arrays[feacture]\n",
    "            x_predict = np.concatenate(( other_feature_array[-sequence_length:,],wind_array[-sequence_length:],data_array[ - sequence_length:]), axis=1)[-sequence_length:];\n",
    "\n",
    "            try:\n",
    "                time_feature = ['Farm', 'year', 'month', 'hour']\n",
    "                wind_features = list(wind_datas[0].drop(columns=['wind_date', 'hors','new_date']).columns);\n",
    "                pre_time = new_data.loc[new_data.index.values[0],'date'] - timedelta(hours=1)\n",
    "                temp_newdata = new_data.copy()\n",
    "                temp_newdata.loc[:,'Farm'] = farmidx + 1\n",
    "                test_X_data = train_data[farmidx][(train_data[farmidx]['date'] <= pre_time) & (\n",
    "                        train_data[farmidx]['date'] > pre_time - timedelta(hours=sequence_length))][farm_features].values;\n",
    "                test_time_X_data = train_data[farmidx][(train_data[farmidx]['date'] <= pre_time + timedelta(hours=1)) & (\n",
    "                        train_data[farmidx]['date'] > pre_time - timedelta(hours=sequence_length - 1))][time_feature].values;\n",
    "                test_time_X_data = np.concatenate( [test_time_X_data,temp_newdata[time_feature].values],axis=0);\n",
    "                test_wind_data = wind_train_Datas[farmidx][(wind_train_Datas[farmidx]['new_date'] <= pre_time + timedelta(hours=1)) &\n",
    "                                           (wind_train_Datas[farmidx]['new_date'] > pre_time - timedelta(hours=sequence_length - 1))][wind_features].values;\n",
    "                test_X_data = np.concatenate([test_time_X_data, test_wind_data, test_X_data], axis=1)\n",
    "                print(\"Test For X_Data RMSE :{}\".format(mean_squared_error(test_X_data, x_predict )))\n",
    "            except Exception as e:\n",
    "                print(e);\n",
    "            if (pd.isnull(np.array(x_predict, dtype=float)).sum() > 0):\n",
    "                print(\"Input NAN {}\".format(idx))\n",
    "\n",
    "            y_predict = model_data[feacture]['model'].model.predict(  np.array([x_predict]))\n",
    "\n",
    "\n",
    "            if (pd.isnull(np.array(y_predict, dtype=float)).sum() > 0):\n",
    "                print(\"Input NAN {}\".format(idx))\n",
    "\n",
    "            new_data.loc[new_data.index._values[0],feacture] = y_predict[0][0]\n",
    "            temp_farm.loc[temp_farm.index.values[0],'power'] = y_predict[0][0]\n",
    "\n",
    "        benchmark_data.loc[benchmark_data['date'] == item] = new_data\n",
    "        # update DataArray\n",
    "        for farmidx, feacture in enumerate(farm_features):\n",
    "            temp_data_array = new_data[ farm_features].values\n",
    "            data_arrays[feacture] = np.concatenate((data_arrays[feacture], temp_data_array), axis=0)\n",
    "            temp_farm = new_data.drop(columns=['id','Flag']).copy()\n",
    "            temp_farm.loc[temp_farm.index._values, 'Farm'] = farmidx + 1\n",
    "            temp_farm.loc[temp_farm.index.values[0], 'power'] = new_data[feacture].values\n",
    "            train_data[farmidx] = pd.concat([train_data[farmidx], temp_farm],sort=False)\n",
    "            train_data[farmidx].sort_values(['date'], ascending=[1], inplace=True)\n",
    "    test_data = benchmark_data.drop(columns=['id','year', 'month', 'hour'])\n",
    "    return test_data\n",
    "\n",
    "def NN_predict_data_group(model_data,res_data,params,Limit_Number=None):\n",
    "    try:\n",
    "        sequence_length = params['sequence_length'] - 1\n",
    "        x_features = res_data['x_features']\n",
    "        farm_features = res_data['farm_feature']\n",
    "        bench_mark = None\n",
    "        # measure process time\n",
    "        t0 = time.clock()\n",
    "        for farm_feature in farm_features:\n",
    "            power_Data = model_data[farm_feature]['power_Data']\n",
    "            predict_datas = power_Data[power_Data['Flag'] == False]\n",
    "            if (Limit_Number is None):\n",
    "                Limit_Number = predict_datas.shape[0]\n",
    "            for index, data_item in tqdm.tqdm(predict_datas[:Limit_Number].iterrows(),total=Limit_Number):\n",
    "                index_loc = index\n",
    "                pred_start_time = power_Data.loc[index_loc-sequence_length,'date']+ timedelta(hours= sequence_length);\n",
    "                start_time = data_item['date']\n",
    "                assert pred_start_time ==start_time\n",
    "                x_predict = np.concatenate([ power_Data[index_loc-sequence_length+1:index_loc+1][x_features[:-1]].values,\n",
    "                                             power_Data[index_loc-sequence_length:index_loc][x_features[-1:]].values],axis=1);\n",
    "                y_predict = model_data[farm_feature]['model'].model.predict(  np.array([x_predict]))\n",
    "                predict_datas.loc[index,'power'] = y_predict[0][0]\n",
    "                power_Data.loc[index_loc,'power'] = y_predict[0][0]\n",
    "            predict_item = predict_datas[['date', 'power']].rename(columns={\"power\": farm_feature})\n",
    "            bench_mark = bench_mark.merge(predict_item, left_on='date',right_on='date') \\\n",
    "                if bench_mark is not None else predict_item\n",
    "        print(\"Time: \",time.clock()-t0)\n",
    "        return bench_mark\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def NN_multi_predict(train_datas,model_data, NN_model):\n",
    "    try:\n",
    "        bench_mark = None\n",
    "        predict_datas = model_data['predict_datas']\n",
    "        for idx,predict_data in enumerate(predict_datas):\n",
    "            farm_feature = 'wp'+str(idx+1)\n",
    "            farm_idx = int(farm_feature[-1])\n",
    "            predict_item = None\n",
    "            #for item in tqdm.tqdm(predict_data):\n",
    "            sub_predict_items = train_datas[(train_datas['Flag']==False)&(train_datas['Farm']== farm_idx )].copy()\n",
    "            y_val = NN_model.model.predict(predict_data)\n",
    "            y_val = y_val.reshape(-1)\n",
    "            print(y_val.shape)\n",
    "            print(sub_predict_items.shape)\n",
    "            print(sub_predict_items.shape)\n",
    "            sub_predict_items['power'] = y_val\n",
    "            predict_item = pd.concat([predict_item, sub_predict_items],axis=0) if predict_item is not None else sub_predict_items\n",
    "            predict_item = predict_item.groupby('date').mean().reset_index()\n",
    "            merge_item = predict_item[['date','power']].copy()\n",
    "            merge_item.rename(columns={\"power\":farm_feature},inplace=True)\n",
    "            bench_mark = bench_mark.merge(merge_item, left_on='date',right_on='date') if bench_mark is not None else merge_item\n",
    "        return bench_mark\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "## 多线程性能没有提高\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "\n",
    "def get_iter_rows(predict_datas,power_Data,sequence_length,model,x_features,farm_feature,Limit_Number=None):\n",
    "    if(Limit_Number is None):\n",
    "        Limit_Number = predict_datas.shape[0]\n",
    "    for index, data_item in predict_datas[:Limit_Number].iterrows(): #tqdm.tqdm(, total=Limit_Number):\n",
    "            index_loc = index\n",
    "            pred_start_time = power_Data.loc[index_loc - sequence_length, 'date'] + timedelta(hours=sequence_length);\n",
    "            start_time = data_item['date']\n",
    "            assert pred_start_time == start_time\n",
    "            x_predict = np.concatenate(\n",
    "                [power_Data[index_loc - sequence_length + 1:index_loc + 1][x_features[:-1]].values,\n",
    "                 power_Data[index_loc - sequence_length:index_loc][x_features[-1:]].values], axis=1)\n",
    "            y_predict = model.predict(np.array([x_predict]))\n",
    "            predict_datas.loc[index, 'power'] = y_predict[0][0]\n",
    "            power_Data.loc[index_loc, 'power'] = y_predict[0][0]\n",
    "    predict_item = predict_datas[['date', 'power']].rename(columns={\"power\": farm_feature})\n",
    "    return predict_item\n",
    "\n",
    "def multi_predict(model_data,res_data,params,Limit_Number=None):\n",
    "    try:\n",
    "        sequence_length = params['sequence_length'] - 1\n",
    "        x_features = res_data['x_features']\n",
    "        farm_features = res_data['farm_feature']\n",
    "        bench_mark = None\n",
    "        pool_size = len(farm_features)\n",
    "        multiple_results=[]\n",
    "        # measure process time\n",
    "        t0 = time.clock()\n",
    "        with Pool(processes= pool_size) as pool:\n",
    "            for farm_feature in (farm_features):\n",
    "                power_Data = model_data[farm_feature]['power_Data']\n",
    "                predict_datas = power_Data[power_Data['Flag'] == False]\n",
    "                model = model_data[farm_feature]['model'].model\n",
    "                multiple_results.append(pool.apply_async(\n",
    "                    get_iter_rows,(power_Data, predict_datas,sequence_length,model,x_features,farm_feature,Limit_Number)));\n",
    "        for res in multiple_results:\n",
    "            predict_item = res.get(); #timeout=1000\n",
    "            bench_mark = bench_mark.merge(predict_item, left_on='date',right_on='date') \\\n",
    "            if bench_mark is not None else predict_item\n",
    "        print(\"Time: \", time.clock() - t0)\n",
    "        return bench_mark\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '/kaggle/input/new_data3.csv'\n",
    "new_data, x_list = get_new_data(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FarmNum = 7\n",
    "#for farm in range(1, FarmNum + 1):\n",
    "#    new_data.loc[new_data['Farm']==farm] = pre_process(new_data[new_data['Farm']==farm],y_list=\"power\",n_clusters = 100, min_sample=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52416, 98)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.loc[new_data['Flag']==False].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:01<00:09,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  1 (13056, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:03<00:07,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  2 (26112, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:04<00:06,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  3 (39168, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:06<00:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  4 (52224, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:08<00:03,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  5 (65280, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:10<00:01,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  6 (78336, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:11<00:00,  1.72s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  7 (91392, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:01<00:11,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  1 (104448, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:03<00:09,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  2 (117504, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:06<00:07,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  3 (130560, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:08<00:06,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  4 (143616, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:10<00:03,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  5 (156672, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:12<00:02,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  6 (169728, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:14<00:00,  2.04s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  7 (182784, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:02<00:13,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  1 (195840, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:04<00:11,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  2 (208896, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:06<00:08,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  3 (221952, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:08<00:06,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  4 (235008, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:11<00:04,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  5 (248064, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:13<00:02,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  6 (261120, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:15<00:00,  2.29s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  7 (274176, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:02<00:14,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  1 (287232, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:04<00:12,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  2 (300288, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:07<00:09,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  3 (313344, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:09<00:07,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  4 (326400, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:12<00:04,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  5 (339456, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:14<00:02,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  6 (352512, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:17<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  7 (365568, 84, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        \"min_thread_value\": 0.00,\n",
    "        \"Limit_Number\": None,\n",
    "        'n_farm': 7,\n",
    "        'thread_value': 0.1,\n",
    "        'number_catagory': 50,\n",
    "        'sequence_length': 37,\n",
    "        'train_set_fraction': 0.90,\n",
    "        'epoch_number': 1,\n",
    "        'seq_interval': 1,\n",
    "        'batch_size': 4096,\n",
    "        'save_dir': 'WIND',  # 保存文件目录;\n",
    "        'objective': 'regression',  # catagory,regression, category,binary_catagory\n",
    "        'add_new_feature': False,\n",
    "        'add_binary': False,\n",
    "        'total': True,\n",
    "        'offset_wind': False,\n",
    "        'NN_model': {\n",
    "            'optimizer': {'type': 'sgd', 'lr': 0.1},\n",
    "            'metrics': {'metrics': ['mse']},  # regression mse, categorical accuracy,\n",
    "            'loss': {'loss': None}  # regression None categorical_crossentropy,binary_crossentropy\n",
    "        },\n",
    "        'layer': [\n",
    "            {\"type\": \"lstm\", \"neurons\": 96, \"input_timesteps\": 84, \"input_dim\": 3, \"return_seq\": True,\"go_backwards\": False},\n",
    "            {\"type\": \"dropout\", \"rate\": 0.4},\n",
    "            #{\"type\": \"lstm\",\"neurons\": 120,\"return_seq\": False},\n",
    "            #{\"type\": \"dropout\",\"rate\": 0.4},\n",
    "            {\"type\": \"lstm\", \"neurons\": 48, \"return_seq\": False},\n",
    "            {\"type\": \"dropout\", \"rate\": 0.4},\n",
    "            #{\"type\": \"lstm\", \"neurons\": 96, \"return_seq\": False},\n",
    "            #{\"type\": \"dropout\", \"rate\": 0.4},\n",
    "            #{'type': 'BatchNormalization'},\n",
    "            #{\"type\": \"dropout\", \"rate\": 0.4},\n",
    "            #{\"type\": \"dense\", \"size\": 48, 'activation': 'linear'},\n",
    "            #{\"type\": \"dropout\", \"rate\": 0.4},\n",
    "            {\"type\": \"dense\", \"size\": 48, 'activation': 'linear'},\n",
    "        ],\n",
    "        'wind_gbm': {'wp1': {'number_leave': 20}, 'wp2': {'number_leave': 50},\n",
    "                     'wp3': {'number_leave': 20}, 'wp4': {'number_leave': 50},\n",
    "                     'wp5': {'number_leave': 50}, 'wp6': {'number_leave': 50},\n",
    "                     'wp7': {'number_leave': 50}},\n",
    "        'binary_gbm': {'wp1': {'number_leave': 100}, 'wp2': {'number_leave': 100},\n",
    "                       'wp3': {'number_leave': 1000}, 'wp4': {'number_leave': 100},\n",
    "                       'wp5': {'number_leave': 200}, 'wp6': {'number_leave': 100},\n",
    "                       'wp7': {'number_leave': 100}},\n",
    "        'gbm_model': {\n",
    "            'is_grid_search': {\n",
    "                'lgm_params': {'num_leaves': [50, 80, 100, 200, 500, 1000, 1500, 2000, 2500, 3000, 4000]}},\n",
    "            'n_estimators': 20000,\n",
    "        },\n",
    "        'file_path': \"GEF2012-wind-forecasting\",  # Kaggle ../input;\n",
    "    }\n",
    "params['epoch_number']= 400\n",
    "#x_list2 = ['Farm','start_catagory','day','hour','dist','ws.angle','ws3.angle',\n",
    "#           'ws2.angle.wp_1','ws2.angle.wp_2','ws2.angle.wp_3','ws2.angle.wp_4',\n",
    "#           'ws2.angle.wp_5','ws2.angle.wp_6','ws2.angle.wp_7','ws.power']\n",
    "\n",
    "x_list2 = ['Farm','start_catagory','hour','dist','ws.angle','ws.power']\n",
    "y_list2 = ['power']\n",
    "NN_Model =wind_power_sequence(x_list2,new_data,params)\n",
    "x_train, y_train,x_test, y_test = NN_Model['x_train'],NN_Model['y_train'],NN_Model['x_test'],NN_Model['y_test']\n",
    "y_train = y_train.reshape(-1, 48)\n",
    "y_test =  y_test.reshape(-1,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365568, 84, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 84, 96)            39552     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 84, 96)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 48)                27840     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 48)                2352      \n",
      "=================================================================\n",
      "Total params: 69,744\n",
      "Trainable params: 69,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[Model] Training Started\n",
      "[Model] 400 epochs, 4096 batch size\n",
      "Train on 365568 samples, validate on 1092 samples\n",
      "Epoch 1/400\n",
      "365568/365568 [==============================] - 34s 93us/step - loss: 0.2956 - mean_squared_error: 0.1059 - root_mean_squared_error: 0.2956 - val_loss: 0.2736 - val_mean_squared_error: 0.0926 - val_root_mean_squared_error: 0.2736\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27358, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 2/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.2410 - mean_squared_error: 0.0693 - root_mean_squared_error: 0.2410 - val_loss: 0.2314 - val_mean_squared_error: 0.0634 - val_root_mean_squared_error: 0.2314\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27358 to 0.23136, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 3/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.2202 - mean_squared_error: 0.0568 - root_mean_squared_error: 0.2202 - val_loss: 0.2222 - val_mean_squared_error: 0.0574 - val_root_mean_squared_error: 0.2222\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23136 to 0.22223, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 4/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.2100 - mean_squared_error: 0.0516 - root_mean_squared_error: 0.2100 - val_loss: 0.2128 - val_mean_squared_error: 0.0519 - val_root_mean_squared_error: 0.2128\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.22223 to 0.21285, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 5/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1994 - mean_squared_error: 0.0463 - root_mean_squared_error: 0.1994 - val_loss: 0.2028 - val_mean_squared_error: 0.0471 - val_root_mean_squared_error: 0.2028\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.21285 to 0.20278, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 6/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1925 - mean_squared_error: 0.0431 - root_mean_squared_error: 0.1925 - val_loss: 0.1946 - val_mean_squared_error: 0.0443 - val_root_mean_squared_error: 0.1946\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.20278 to 0.19465, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 7/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1879 - mean_squared_error: 0.0411 - root_mean_squared_error: 0.1879 - val_loss: 0.1929 - val_mean_squared_error: 0.0431 - val_root_mean_squared_error: 0.1929\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.19465 to 0.19289, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 8/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1843 - mean_squared_error: 0.0395 - root_mean_squared_error: 0.1843 - val_loss: 0.1910 - val_mean_squared_error: 0.0426 - val_root_mean_squared_error: 0.1910\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.19289 to 0.19104, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 9/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1813 - mean_squared_error: 0.0382 - root_mean_squared_error: 0.1813 - val_loss: 0.1834 - val_mean_squared_error: 0.0391 - val_root_mean_squared_error: 0.1834\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.19104 to 0.18338, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 10/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1783 - mean_squared_error: 0.0369 - root_mean_squared_error: 0.1783 - val_loss: 0.1808 - val_mean_squared_error: 0.0381 - val_root_mean_squared_error: 0.1808\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.18338 to 0.18075, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 11/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1755 - mean_squared_error: 0.0358 - root_mean_squared_error: 0.1755 - val_loss: 0.1759 - val_mean_squared_error: 0.0361 - val_root_mean_squared_error: 0.1759\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.18075 to 0.17595, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 12/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1734 - mean_squared_error: 0.0349 - root_mean_squared_error: 0.1734 - val_loss: 0.1765 - val_mean_squared_error: 0.0362 - val_root_mean_squared_error: 0.1765\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.17595\n",
      "Epoch 13/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1710 - mean_squared_error: 0.0339 - root_mean_squared_error: 0.1710 - val_loss: 0.1710 - val_mean_squared_error: 0.0341 - val_root_mean_squared_error: 0.1710\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.17595 to 0.17104, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 14/400\n",
      "365568/365568 [==============================] - 31s 86us/step - loss: 0.1691 - mean_squared_error: 0.0332 - root_mean_squared_error: 0.1691 - val_loss: 0.1716 - val_mean_squared_error: 0.0343 - val_root_mean_squared_error: 0.1716\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.17104\n",
      "Epoch 15/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1684 - mean_squared_error: 0.0329 - root_mean_squared_error: 0.1684 - val_loss: 0.1692 - val_mean_squared_error: 0.0334 - val_root_mean_squared_error: 0.1692\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.17104 to 0.16919, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 16/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1665 - mean_squared_error: 0.0322 - root_mean_squared_error: 0.1665 - val_loss: 0.1688 - val_mean_squared_error: 0.0331 - val_root_mean_squared_error: 0.1688\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.16919 to 0.16885, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 17/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1655 - mean_squared_error: 0.0317 - root_mean_squared_error: 0.1655 - val_loss: 0.1657 - val_mean_squared_error: 0.0321 - val_root_mean_squared_error: 0.1657\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.16885 to 0.16569, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 18/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1640 - mean_squared_error: 0.0312 - root_mean_squared_error: 0.1640 - val_loss: 0.1670 - val_mean_squared_error: 0.0323 - val_root_mean_squared_error: 0.1670\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.16569\n",
      "Epoch 19/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1631 - mean_squared_error: 0.0309 - root_mean_squared_error: 0.1631 - val_loss: 0.1645 - val_mean_squared_error: 0.0316 - val_root_mean_squared_error: 0.1645\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.16569 to 0.16451, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 20/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1619 - mean_squared_error: 0.0304 - root_mean_squared_error: 0.1619 - val_loss: 0.1644 - val_mean_squared_error: 0.0316 - val_root_mean_squared_error: 0.1644\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.16451 to 0.16439, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 21/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1611 - mean_squared_error: 0.0301 - root_mean_squared_error: 0.1611 - val_loss: 0.1660 - val_mean_squared_error: 0.0321 - val_root_mean_squared_error: 0.1660\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.16439\n",
      "Epoch 22/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1602 - mean_squared_error: 0.0297 - root_mean_squared_error: 0.1602 - val_loss: 0.1638 - val_mean_squared_error: 0.0313 - val_root_mean_squared_error: 0.1638\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.16439 to 0.16383, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 23/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1591 - mean_squared_error: 0.0293 - root_mean_squared_error: 0.1591 - val_loss: 0.1613 - val_mean_squared_error: 0.0302 - val_root_mean_squared_error: 0.1613\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.16383 to 0.16131, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 24/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1582 - mean_squared_error: 0.0290 - root_mean_squared_error: 0.1582 - val_loss: 0.1633 - val_mean_squared_error: 0.0309 - val_root_mean_squared_error: 0.1633\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.16131\n",
      "Epoch 25/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1576 - mean_squared_error: 0.0288 - root_mean_squared_error: 0.1576 - val_loss: 0.1617 - val_mean_squared_error: 0.0304 - val_root_mean_squared_error: 0.1617\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.16131\n",
      "Epoch 26/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1564 - mean_squared_error: 0.0283 - root_mean_squared_error: 0.1564 - val_loss: 0.1639 - val_mean_squared_error: 0.0311 - val_root_mean_squared_error: 0.1639\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.16131\n",
      "Epoch 27/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1558 - mean_squared_error: 0.0281 - root_mean_squared_error: 0.1558 - val_loss: 0.1614 - val_mean_squared_error: 0.0303 - val_root_mean_squared_error: 0.1614\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.16131\n",
      "Epoch 28/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1551 - mean_squared_error: 0.0278 - root_mean_squared_error: 0.1551 - val_loss: 0.1581 - val_mean_squared_error: 0.0290 - val_root_mean_squared_error: 0.1581\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.16131 to 0.15809, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 29/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1541 - mean_squared_error: 0.0275 - root_mean_squared_error: 0.1541 - val_loss: 0.1606 - val_mean_squared_error: 0.0298 - val_root_mean_squared_error: 0.1606\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.15809\n",
      "Epoch 30/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1536 - mean_squared_error: 0.0273 - root_mean_squared_error: 0.1536 - val_loss: 0.1585 - val_mean_squared_error: 0.0292 - val_root_mean_squared_error: 0.1585\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15809\n",
      "Epoch 31/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1530 - mean_squared_error: 0.0271 - root_mean_squared_error: 0.1530 - val_loss: 0.1582 - val_mean_squared_error: 0.0289 - val_root_mean_squared_error: 0.1582\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.15809\n",
      "Epoch 32/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1519 - mean_squared_error: 0.0267 - root_mean_squared_error: 0.1519 - val_loss: 0.1564 - val_mean_squared_error: 0.0283 - val_root_mean_squared_error: 0.1564\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.15809 to 0.15637, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 33/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1514 - mean_squared_error: 0.0265 - root_mean_squared_error: 0.1514 - val_loss: 0.1570 - val_mean_squared_error: 0.0285 - val_root_mean_squared_error: 0.1570\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.15637\n",
      "Epoch 34/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1510 - mean_squared_error: 0.0263 - root_mean_squared_error: 0.1510 - val_loss: 0.1567 - val_mean_squared_error: 0.0284 - val_root_mean_squared_error: 0.1567\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.15637\n",
      "Epoch 35/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1501 - mean_squared_error: 0.0261 - root_mean_squared_error: 0.1501 - val_loss: 0.1572 - val_mean_squared_error: 0.0286 - val_root_mean_squared_error: 0.1572\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.15637\n",
      "Epoch 36/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1496 - mean_squared_error: 0.0258 - root_mean_squared_error: 0.1496 - val_loss: 0.1542 - val_mean_squared_error: 0.0276 - val_root_mean_squared_error: 0.1542\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.15637 to 0.15420, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 37/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1489 - mean_squared_error: 0.0256 - root_mean_squared_error: 0.1489 - val_loss: 0.1583 - val_mean_squared_error: 0.0290 - val_root_mean_squared_error: 0.1583\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.15420\n",
      "Epoch 38/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1485 - mean_squared_error: 0.0255 - root_mean_squared_error: 0.1485 - val_loss: 0.1542 - val_mean_squared_error: 0.0275 - val_root_mean_squared_error: 0.1542\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.15420 to 0.15416, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 39/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1476 - mean_squared_error: 0.0252 - root_mean_squared_error: 0.1476 - val_loss: 0.1541 - val_mean_squared_error: 0.0274 - val_root_mean_squared_error: 0.1541\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.15416 to 0.15412, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 40/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1474 - mean_squared_error: 0.0251 - root_mean_squared_error: 0.1474 - val_loss: 0.1588 - val_mean_squared_error: 0.0291 - val_root_mean_squared_error: 0.1588\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.15412\n",
      "Epoch 41/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1470 - mean_squared_error: 0.0249 - root_mean_squared_error: 0.1470 - val_loss: 0.1542 - val_mean_squared_error: 0.0276 - val_root_mean_squared_error: 0.1542\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.15412\n",
      "Epoch 42/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1465 - mean_squared_error: 0.0248 - root_mean_squared_error: 0.1465 - val_loss: 0.1556 - val_mean_squared_error: 0.0279 - val_root_mean_squared_error: 0.1556\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.15412\n",
      "Epoch 43/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1459 - mean_squared_error: 0.0246 - root_mean_squared_error: 0.1459 - val_loss: 0.1531 - val_mean_squared_error: 0.0271 - val_root_mean_squared_error: 0.1531\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.15412 to 0.15315, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 44/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1455 - mean_squared_error: 0.0244 - root_mean_squared_error: 0.1455 - val_loss: 0.1524 - val_mean_squared_error: 0.0269 - val_root_mean_squared_error: 0.1524\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.15315 to 0.15239, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 45/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1454 - mean_squared_error: 0.0244 - root_mean_squared_error: 0.1454 - val_loss: 0.1524 - val_mean_squared_error: 0.0268 - val_root_mean_squared_error: 0.1524\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.15239 to 0.15237, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 46/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1450 - mean_squared_error: 0.0243 - root_mean_squared_error: 0.1450 - val_loss: 0.1518 - val_mean_squared_error: 0.0267 - val_root_mean_squared_error: 0.1518\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.15237 to 0.15182, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 47/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1446 - mean_squared_error: 0.0241 - root_mean_squared_error: 0.1446 - val_loss: 0.1543 - val_mean_squared_error: 0.0274 - val_root_mean_squared_error: 0.1543\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.15182\n",
      "Epoch 48/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1444 - mean_squared_error: 0.0240 - root_mean_squared_error: 0.1444 - val_loss: 0.1517 - val_mean_squared_error: 0.0266 - val_root_mean_squared_error: 0.1517\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.15182 to 0.15172, saving model to WIND/09042020-013147_wsPower2.hdf5\n",
      "Epoch 49/400\n",
      " 98304/365568 [=======>......................] - ETA: 21s - loss: 0.1444 - mean_squared_error: 0.0241 - root_mean_squared_error: 0.1444"
     ]
    }
   ],
   "source": [
    "params['layer'][0]['input_dim'] =  x_train.shape[-1]\n",
    "Net_model = NN_Net(params,x_train)\n",
    "scores = Net_model.train( x_train, y_train, x_test,y_test,'wsPower2', load_models=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测误差：  0.14272394427013047\n"
     ]
    }
   ],
   "source": [
    "print(\"预测误差： \",scores[-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Training Started\n",
      "[Model] 400 epochs, 4096 batch size\n",
      "Train on 365568 samples, validate on 1092 samples\n",
      "Epoch 1/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1298 - mean_squared_error: 0.0194 - root_mean_squared_error: 0.1298 - val_loss: 0.1440 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1440\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14403, saving model to WIND/09042020-045302_wsPower2.hdf5\n",
      "Epoch 2/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1299 - mean_squared_error: 0.0194 - root_mean_squared_error: 0.1299 - val_loss: 0.1430 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1430\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14403 to 0.14304, saving model to WIND/09042020-045302_wsPower2.hdf5\n",
      "Epoch 3/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1298 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1298 - val_loss: 0.1442 - val_mean_squared_error: 0.0242 - val_root_mean_squared_error: 0.1442\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14304\n",
      "Epoch 4/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1298 - mean_squared_error: 0.0194 - root_mean_squared_error: 0.1298 - val_loss: 0.1452 - val_mean_squared_error: 0.0245 - val_root_mean_squared_error: 0.1452\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.14304\n",
      "Epoch 5/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1298 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1298 - val_loss: 0.1446 - val_mean_squared_error: 0.0243 - val_root_mean_squared_error: 0.1446\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.14304\n",
      "Epoch 6/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1298 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1298 - val_loss: 0.1440 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1440\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.14304\n",
      "Epoch 7/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1298 - mean_squared_error: 0.0194 - root_mean_squared_error: 0.1298 - val_loss: 0.1432 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1432\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.14304\n",
      "Epoch 8/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1298 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1298 - val_loss: 0.1434 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1434\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.14304\n",
      "Epoch 9/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1298 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1298 - val_loss: 0.1440 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1440\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.14304\n",
      "Epoch 10/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1429 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1429\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.14304 to 0.14292, saving model to WIND/09042020-045302_wsPower2.hdf5\n",
      "Epoch 11/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1433 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1433\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14292\n",
      "Epoch 12/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1434 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1434\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14292\n",
      "Epoch 13/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1298 - mean_squared_error: 0.0194 - root_mean_squared_error: 0.1298 - val_loss: 0.1441 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1441\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.14292\n",
      "Epoch 14/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1447 - val_mean_squared_error: 0.0243 - val_root_mean_squared_error: 0.1447\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.14292\n",
      "Epoch 15/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1449 - val_mean_squared_error: 0.0244 - val_root_mean_squared_error: 0.1449\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.14292\n",
      "Epoch 16/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1431 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1431\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.14292\n",
      "Epoch 17/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1440 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1440\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.14292\n",
      "Epoch 18/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1447 - val_mean_squared_error: 0.0243 - val_root_mean_squared_error: 0.1447\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.14292\n",
      "Epoch 19/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1444 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1444\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.14292\n",
      "Epoch 20/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1432 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1432\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.14292\n",
      "Epoch 21/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1432 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1432\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.14292\n",
      "Epoch 22/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1298 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1298 - val_loss: 0.1446 - val_mean_squared_error: 0.0243 - val_root_mean_squared_error: 0.1446\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.14292\n",
      "Epoch 23/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1432 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1432\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.14292\n",
      "Epoch 24/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1434 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1434\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.14292\n",
      "Epoch 25/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1437 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1437\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.14292\n",
      "Epoch 26/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1441 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1441\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.14292\n",
      "Epoch 27/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1436 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1436\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.14292\n",
      "Epoch 28/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1434 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1434\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.14292\n",
      "Epoch 29/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1444 - val_mean_squared_error: 0.0242 - val_root_mean_squared_error: 0.1444\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.14292\n",
      "Epoch 30/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1440 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1440\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.14292\n",
      "Epoch 31/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1297 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1297 - val_loss: 0.1437 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1437\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.14292\n",
      "Epoch 32/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1448 - val_mean_squared_error: 0.0244 - val_root_mean_squared_error: 0.1448\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.14292\n",
      "Epoch 33/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1434 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1434\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.14292\n",
      "Epoch 34/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1456 - val_mean_squared_error: 0.0246 - val_root_mean_squared_error: 0.1456\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.14292\n",
      "Epoch 35/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1429 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1429\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.14292\n",
      "Epoch 36/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1442 - val_mean_squared_error: 0.0242 - val_root_mean_squared_error: 0.1442\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.14292\n",
      "Epoch 37/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1436 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1436\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.14292\n",
      "Epoch 38/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1433 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1433\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.14292\n",
      "Epoch 39/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1446 - val_mean_squared_error: 0.0243 - val_root_mean_squared_error: 0.1446\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.14292\n",
      "Epoch 40/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1453 - val_mean_squared_error: 0.0245 - val_root_mean_squared_error: 0.1453\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.14292\n",
      "Epoch 41/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1444 - val_mean_squared_error: 0.0242 - val_root_mean_squared_error: 0.1444\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.14292\n",
      "Epoch 42/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1295 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1295 - val_loss: 0.1435 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1435\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.14292\n",
      "Epoch 43/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1442 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1442\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.14292\n",
      "Epoch 44/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1436 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1436\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.14292\n",
      "Epoch 45/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1442 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1442\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.14292\n",
      "Epoch 46/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1428 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1428\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.14292 to 0.14276, saving model to WIND/09042020-045302_wsPower2.hdf5\n",
      "Epoch 47/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1296 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1296 - val_loss: 0.1430 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1430\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.14276\n",
      "Epoch 48/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1447 - val_mean_squared_error: 0.0243 - val_root_mean_squared_error: 0.1447\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.14276\n",
      "Epoch 49/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1430 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1430\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.14276\n",
      "Epoch 50/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1439 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1439\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.14276\n",
      "Epoch 51/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1443 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1443\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.14276\n",
      "Epoch 52/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1436 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1436\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.14276\n",
      "Epoch 53/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1433 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1433\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.14276\n",
      "Epoch 54/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1440 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1440\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.14276\n",
      "Epoch 55/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1431 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1431\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.14276\n",
      "Epoch 56/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1426 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1426\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.14276 to 0.14264, saving model to WIND/09042020-045302_wsPower2.hdf5\n",
      "Epoch 57/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1433 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1433\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.14264\n",
      "Epoch 58/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1431 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1431\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.14264\n",
      "Epoch 59/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1429 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1429\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.14264\n",
      "Epoch 60/400\n",
      "365568/365568 [==============================] - 31s 85us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1448 - val_mean_squared_error: 0.0243 - val_root_mean_squared_error: 0.1448\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.14264\n",
      "Epoch 61/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1295 - mean_squared_error: 0.0193 - root_mean_squared_error: 0.1295 - val_loss: 0.1432 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1432\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.14264\n",
      "Epoch 62/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1429 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1429\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.14264\n",
      "Epoch 63/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1428 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1428\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.14264\n",
      "Epoch 64/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1437 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1437\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.14264\n",
      "Epoch 65/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1457 - val_mean_squared_error: 0.0246 - val_root_mean_squared_error: 0.1457\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.14264\n",
      "Epoch 66/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1443 - val_mean_squared_error: 0.0242 - val_root_mean_squared_error: 0.1443\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.14264\n",
      "Epoch 67/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1446 - val_mean_squared_error: 0.0243 - val_root_mean_squared_error: 0.1446\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.14264\n",
      "Epoch 68/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1443 - val_mean_squared_error: 0.0242 - val_root_mean_squared_error: 0.1443\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.14264\n",
      "Epoch 69/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1445 - val_mean_squared_error: 0.0242 - val_root_mean_squared_error: 0.1445\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.14264\n",
      "Epoch 70/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1428 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1428\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.14264\n",
      "Epoch 71/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1440 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1440\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.14264\n",
      "Epoch 72/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1294 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1294 - val_loss: 0.1457 - val_mean_squared_error: 0.0246 - val_root_mean_squared_error: 0.1457\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.14264\n",
      "Epoch 73/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1433 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1433\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.14264\n",
      "Epoch 74/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1434 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1434\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.14264\n",
      "Epoch 75/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1425 - val_mean_squared_error: 0.0236 - val_root_mean_squared_error: 0.1425\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.14264 to 0.14251, saving model to WIND/09042020-045302_wsPower2.hdf5\n",
      "Epoch 76/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1435 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1435\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.14251\n",
      "Epoch 77/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1428 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1428\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.14251\n",
      "Epoch 78/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1441 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1441\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.14251\n",
      "Epoch 79/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1450 - val_mean_squared_error: 0.0244 - val_root_mean_squared_error: 0.1450\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.14251\n",
      "Epoch 80/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1455 - val_mean_squared_error: 0.0245 - val_root_mean_squared_error: 0.1455\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.14251\n",
      "Epoch 81/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1444 - val_mean_squared_error: 0.0242 - val_root_mean_squared_error: 0.1444\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.14251\n",
      "Epoch 82/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1438 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1438\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.14251\n",
      "Epoch 83/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1439 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1439\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.14251\n",
      "Epoch 84/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1428 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1428\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.14251\n",
      "Epoch 85/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1435 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1435\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.14251\n",
      "Epoch 86/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1432 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1432\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.14251\n",
      "Epoch 87/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1453 - val_mean_squared_error: 0.0245 - val_root_mean_squared_error: 0.1453\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.14251\n",
      "Epoch 88/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1431 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1431\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.14251\n",
      "Epoch 89/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1443 - val_mean_squared_error: 0.0242 - val_root_mean_squared_error: 0.1443\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.14251\n",
      "Epoch 90/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1293 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1293 - val_loss: 0.1430 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1430\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.14251\n",
      "Epoch 91/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1426 - val_mean_squared_error: 0.0236 - val_root_mean_squared_error: 0.1426\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.14251\n",
      "Epoch 92/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1433 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1433\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.14251\n",
      "Epoch 93/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1431 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1431\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.14251\n",
      "Epoch 94/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1441 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1441\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.14251\n",
      "Epoch 95/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1434 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1434\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.14251\n",
      "Epoch 96/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1432 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1432\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.14251\n",
      "Epoch 97/400\n",
      "365568/365568 [==============================] - 30s 81us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1436 - val_mean_squared_error: 0.0239 - val_root_mean_squared_error: 0.1436\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.14251\n",
      "Epoch 98/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1425 - val_mean_squared_error: 0.0236 - val_root_mean_squared_error: 0.1425\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.14251\n",
      "Epoch 99/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1291 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1291 - val_loss: 0.1438 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1438\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.14251\n",
      "Epoch 100/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1439 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1439\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.14251\n",
      "Epoch 101/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1292 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1292 - val_loss: 0.1425 - val_mean_squared_error: 0.0236 - val_root_mean_squared_error: 0.1425\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.14251\n",
      "Epoch 102/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1291 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1291 - val_loss: 0.1439 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1439\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.14251\n",
      "Epoch 103/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1290 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1290 - val_loss: 0.1438 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1438\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.14251\n",
      "Epoch 104/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1291 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1291 - val_loss: 0.1440 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1440\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.14251\n",
      "Epoch 105/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1291 - mean_squared_error: 0.0192 - root_mean_squared_error: 0.1291 - val_loss: 0.1430 - val_mean_squared_error: 0.0238 - val_root_mean_squared_error: 0.1430\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.14251\n",
      "Epoch 106/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1291 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1291 - val_loss: 0.1437 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1437\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.14251\n",
      "Epoch 107/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1291 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1291 - val_loss: 0.1428 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1428\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.14251\n",
      "Epoch 108/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1291 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1291 - val_loss: 0.1442 - val_mean_squared_error: 0.0241 - val_root_mean_squared_error: 0.1442\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.14251\n",
      "Epoch 109/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1290 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1290 - val_loss: 0.1435 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1435\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.14251\n",
      "Epoch 110/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1291 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1291 - val_loss: 0.1453 - val_mean_squared_error: 0.0244 - val_root_mean_squared_error: 0.1453\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.14251\n",
      "Epoch 111/400\n",
      "365568/365568 [==============================] - 31s 84us/step - loss: 0.1291 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1291 - val_loss: 0.1429 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1429\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.14251\n",
      "Epoch 112/400\n",
      "365568/365568 [==============================] - 30s 82us/step - loss: 0.1290 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1290 - val_loss: 0.1426 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1426\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.14251\n",
      "Epoch 113/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1290 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1290 - val_loss: 0.1428 - val_mean_squared_error: 0.0237 - val_root_mean_squared_error: 0.1428\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.14251\n",
      "Epoch 114/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1290 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1290 - val_loss: 0.1436 - val_mean_squared_error: 0.0240 - val_root_mean_squared_error: 0.1436\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.14251\n",
      "Epoch 115/400\n",
      "365568/365568 [==============================] - 30s 83us/step - loss: 0.1290 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1290 - val_loss: 0.1451 - val_mean_squared_error: 0.0242 - val_root_mean_squared_error: 0.1451\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.14251\n",
      "Epoch 116/400\n",
      "110592/365568 [========>.....................] - ETA: 21s - loss: 0.1291 - mean_squared_error: 0.0191 - root_mean_squared_error: 0.1291"
     ]
    }
   ],
   "source": [
    "scores =Net_model.train( x_train, y_train, x_test,y_test,'wsPower2', load_models=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Error：  0.1415217773163275\n"
     ]
    }
   ],
   "source": [
    "print(\"Predict RMSE Error ： \",scores[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Net_model.model.evaluate(x_test, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After get the predict model ,\n",
    "# Post Processing using Physical Boundary Constrains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
